---
title: 机器学习算法概览
date: 2018-11-27 13:04:27
categories: [机器学习]
tags: [机器学习]
---

这篇文章我们将讲解一下**机器学习与数据挖掘领域**最常用的一些算法，包括**有监督学习**的**分类与回归**算法以及**无监督学习**的**聚类与降维**算法。好了，闲话不多说，让我们一起开始吧！
<!--more-->

---

### K-Nearest Neighbor(K近邻)

#### 前置假设

* 输入空间: $\chi \subseteq R^n$
* 训练集: $T = [(x_1,y_1),(x_2,y_2),...,(x_N,y_N)]$
* 输出空间: $\gamma = [c_1,c_2,...,c_k]$


#### K近邻算法基本思想

K近邻算法并**不具有显式的学习过程**，K近邻算法实际上是**利用训练集数据对特征空间进行划分**，并作为其分类的模型。给定一个训练集，对于新的输入实例，首先在训练集中找到与该实例最近的K个实例，然后统计这K个实例的多数属于哪个类，就把该实例划分为哪个类。


#### K近邻算法的三个要素

经过上面的讨论，我们可以总结出K近邻算法的三个要素：

* K值的选择
* 距离的度量
* 分类决策的准则

##### K值的选择

首先考虑一个极端的情况，当K值为1时，此时的K近邻算法又称为最近邻算法，这种情况下，很容易发生**过拟合**，很容易将一些噪声学习到模型中(很容易将实例判定为噪声类别)

我们再考虑另外一种极端情况，当K值为N时，此时不管你输入的实例是什么类别，最终的模型都会将该实例判定为模型中实例最多的类别。也就是在这种情况下，很容易发生**欠拟合**。

##### 距离的度量

* 闵可夫斯基距离
* 曼哈顿距离
* 欧氏距离
* 切比雪夫距离

设有两个向量：

$$x_i = (x_{i}^{(1)},x_{i}^{(2)},x_{i}^{(3)},...,x_{i}^{(n)})$$

$$x_j = (x_{j}^{(1)},x_{j}^{(2)},x_{j}^{(3)},...,x_{j}^{(n)})$$

闵可夫斯基距离的定义如下：

$$d_{(x_i,x_j)} = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1 \over p}$$

当 p=1 时就是曼哈顿距离，当 p=2 时就是欧氏距离，当 p=$\infty$时就是切比雪夫距离。

##### 分类决策的准则

这利用的是**多数表决**的决策准则，关于对多数表决规则的解释可以参考《统计学习方法》这本书的3.2.4小节(**多数表决规则等价于经验风险最小化**)


#### 特征归一化

为了让各个特征在分类的时候同等重要，我们需要将各个特征进行归一化。


#### 对异常值敏感

由于KNN是**基于距离**的算法，所以KNN对异常值是比较敏感的。

---

### Linear Regression(线性回归)

#### 定义符号

