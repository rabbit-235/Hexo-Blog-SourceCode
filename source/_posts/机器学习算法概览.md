---
title: 机器学习算法概览
date: 2018-11-27 13:04:27
categories: [机器学习]
tags: [机器学习]
---

这篇文章我们将讲解一下**机器学习与数据挖掘领域**最常用的一些算法，包括**有监督学习**的**分类与回归**算法以及**无监督学习**的**聚类与降维**算法。好了，闲话不多说，让我们一起开始吧！
<!--more-->

---

### K-Nearest Neighbor(K近邻)

#### 前置假设

* 输入空间: $\chi \subseteq \mathbb R^n$
* 训练集: $T = [(x_1,y_1),(x_2,y_2),...,(x_N,y_N)]$
* 输出空间: $\gamma = [c_1,c_2,...,c_k]$


#### K近邻算法基本思想

K近邻算法并**不具有显式的学习过程**，K近邻算法实际上是**利用训练集数据对特征空间进行划分**，并作为其分类的模型。给定一个训练集，对于新的输入实例，首先在训练集中找到与该实例最近的K个实例，然后统计这K个实例的多数属于哪个类，就把该实例划分为哪个类。


#### K近邻算法的三个要素

经过上面的讨论，我们可以总结出K近邻算法的三个要素：

* K值的选择
* 距离的度量
* 分类决策的准则

##### K值的选择

首先考虑一个极端的情况，当K值为1时，此时的K近邻算法又称为最近邻算法，这种情况下，很容易发生**过拟合**，很容易将一些噪声学习到模型中(很容易将实例判定为噪声类别)

我们再考虑另外一种极端情况，当K值为N时，此时不管你输入的实例是什么类别，最终的模型都会将该实例判定为模型中实例最多的类别。也就是在这种情况下，很容易发生**欠拟合**。

##### 距离的度量

* 闵可夫斯基距离
* 曼哈顿距离
* 欧氏距离
* 切比雪夫距离

设有两个向量：

$$x_i = (x_{i}^{(1)},x_{i}^{(2)},x_{i}^{(3)},...,x_{i}^{(n)})$$

$$x_j = (x_{j}^{(1)},x_{j}^{(2)},x_{j}^{(3)},...,x_{j}^{(n)})$$

闵可夫斯基距离的定义如下：

$$d_{(x_i,x_j)} = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{1 \over p}$$

当 p=1 时就是曼哈顿距离，当 p=2 时就是欧氏距离，当 p=$\infty$时就是切比雪夫距离。

##### 分类决策的准则

这利用的是**多数表决**的决策准则，关于对多数表决规则的解释可以参考《统计学习方法》这本书的3.2.4小节(**多数表决规则等价于经验风险最小化**)


#### 特征归一化

为了让各个特征在分类的时候同等重要，我们需要将各个特征进行归一化。


#### 对异常值敏感

由于KNN是**基于距离**的算法，所以KNN对异常值是比较敏感的。

---

### Linear Regression(线性回归)

#### 定义符号

假设数据集为：{$(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})$}，参数为$\theta$，其中：

* 训练样本数目为 m
* 第 i 个样本为$(x^{(i)},y^{(i)})$
* $x^{(i)} = [x_1^{(i)},x_2^{(i)},...,x_n^{(i)}]^T, x^{(i)} \in \mathbb R^n$
* $y^{(i)} \in \mathbb R$
* $\theta = [\theta_1,\theta_2,...,\theta_n]^T, \theta \in \mathbb R^n$


#### 假设函数

$$h(x) = \sum_{i=1}^n \theta_ix_i = \theta^Tx$$


#### 目标函数

##### 目标函数的形式

$$J(\theta) = {1 \over 2m}\sum_{i=1}^m(h_{\theta}(x^{(i)})-y^{(i)})^2$$

##### 为什么要选择这样的目标函数？

(1) 对于每一个样例$(x^{(i)},y^{(i)})$, 假设预测值和真实值存在以下关系：

$$y^{(i)} = {\theta}^Tx^{(i)} + {\epsilon}^{(i)}$$

其中${\epsilon}^{(i)}$表示预测值和真实值之间的差值。

(2) 影响误差的因素有很多，这些因素又都是随机分布的。根据中心极限定理，许多独立随机变量的和趋于正态分布。所以进一步假设：

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/1.png">
</div>

当给定参数$\theta$和变量$x$时，有以下公式成立：

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/2.png">
</div>

(3) 再进一步假设各个样例的误差是独立同分布的，可以得到似然函数：

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/3.png">
</div>

因为似然函数表示的是在参数$\theta$下数据集出现的概率，所以需要做的工作就是极大化似然函数。

(4) 将似然函数转化为对数似然：

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/4.png">
</div>

转换为对数似然函数后，需要做的工作也转变为极大化对数似然函数，要极大化对数似然函数，从式子中可以得出需要使得$\sum_{i=1}^m(\theta^Tx^{(i)} - y^{(i)})^2$最小。到这一步也基本可以对选择这样的目标函数做出一个比较合理的解释了。


#### 优化目标函数的方法

##### 批量梯度下降

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/5.png">
</div>

##### 随机梯度下降

当每次只用一个样本训练时，$J(\theta)$退化成以下形式：

$$J(\theta) = (h_\theta(x^{(i)})-y^{(i)})^2$$

此时参数更新公式变为以下形式：

$$\theta_j:= \theta_j - \alpha(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$

随机梯度下降(Stochastic Gradient Descent)可能永远不能收敛到最小值，参数$\theta$将会一直在使得$J(\theta)$取最小值的附近振荡。

---

### K-means(K-均值聚类)

#### 假设

K-means算法是一种聚类算法。为了更好的解释这个算法，首先我们假设给定的数据集为$(x^{(1)},x^{(2)},...,x^{(m)}),x^{(i)} \in \mathbb R^n,$, 注意**数据是没有标签的**。


#### K-means算法的一般流程

1. 选择初始的K个聚类中心$\mu_1,\mu_2,...,\mu_k \in \mathbb R^n$
2. 重复以下两步直到收敛(聚类中心不再变化或者变化低于阈值)：

(1) 计算每个样本到各个聚类中心的距离，并将其类别标号设为距其最近的聚类中心的标号，即：

$$c^{(i)}:=arg\min_{j}||x^{(i)}-\mu^{(j)}||^2, j = 1,2,...,k$$

其中$c^{(i)}$为第i个样例的类别标号

(2) 更新聚类中心的值为相同类别的样本的平均值：

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/6.png">
</div>

其中，当$c^{(i)} = j$时，$I\{c^{(i)}=j\} = 1$,当$c^{(i)} \ne j$时，$I\{c^{(i)}=j\} = 0$


#### 对K-means算法的进一步解释

1. K-means算法要优化的目标函数如下：

<div align=center>
<img src ="https://myblogs-photos-1256941622.cos.ap-chengdu.myqcloud.com/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E6%A6%82%E8%A7%88/7.png">
</div>


优化目标可以看成让所有点到其对应的聚类中心点的距离和最小。K-means算法可以看成对目标函数J的**坐标下降**过程，对应的解释如下：

* 执行上述2(1)这一步的时候，相当于固定$\mu$,改变$c$ (每个样本所对应的类别)，改变的规则是样本到哪个聚类中心的距离最小就将对应的样本对应的$c$改为哪类，所以$J(c,\mu)$一定会减小。
* 执行上述2(2)步的时候，相当于固定所有样本的$c$,重新计算各个类别的中心，进一步使得$J(c,\mu)$减小。

2. 目标函数$J$不是一个凸函数，因此K-means算法不能保证收敛到全局最优解，一个简单的方法就是随机初始化多次，以最优的聚类结果作为最终的结果。
3. 聚类结束后，如果一个中心没有任何相关的样本，那么这个中心就应该去掉，或者重新聚类。


#### 对K-means算法的改进

1. K-means++算法对K个初始聚类中心的选取做了改进，各个聚类中心之间的距离越远越好。

(1) 随机选取一个聚类中心
(2) 计算每个样本到所有已知聚类中心的最短距离$D(x)$
(3) 计算每个样本被选为下一个聚类中心的概率${D^2(x)}\over{\sum_{x \in X}D^2(x)}$
(4) 确定每个样本被选为下一个聚类中心的概率区间
(5) 生成一个0~1的随机数，选取随机数对应区间的样本作为下一个聚类中心
(6) 重复以上过程，直到选取了K个聚类中心

2. 二分K-means算法，为了克服K-means聚类算法容易陷入局部最小值的问题和提高聚类的性能，提出了二分K-means聚类算法。该算法的基本思想是首先将所有的样本点划分到一个簇，然后将该簇一分为二，之后选择其中的一个簇继续进行划分，直到得到用户指定的簇数目为止。选择哪个簇进行划分取决于对其划分是否可以最大程度的降低SSE(误差平方和)


#### K值的选取

对于如何选取K值，本文只简要提及以下两种方法：

* 经验法
* 手肘法：横坐标为K值，纵坐标为所有样本点到它所对应的聚类中心的误差平方和，在图中找到最佳拐点。

---

